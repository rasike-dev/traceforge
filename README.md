# TraceForge ğŸ”¥

> **The Complete AI Orchestration Platform with Built-in Observability, Evaluation, and Remediation**

TraceForge is a production-ready, enterprise-grade orchestration framework for building reliable AI applications. Unlike traditional AI orchestration tools, TraceForge provides **deep observability**, **automatic quality evaluation**, and **intelligent remediation** out of the boxâ€”all while orchestrating RAG, LLM, and tool calls.

[![TypeScript](https://img.shields.io/badge/TypeScript-5.9-blue.svg)](https://www.typescriptlang.org/)
[![NestJS](https://img.shields.io/badge/NestJS-11.0-E0234E.svg)](https://nestjs.com/)
[![pnpm](https://img.shields.io/badge/pnpm-10.26-F69220.svg)](https://pnpm.io/)
[![OpenTelemetry](https://img.shields.io/badge/OpenTelemetry-enabled-000000.svg)](https://opentelemetry.io/)

---

## ğŸ¯ Why TraceForge?

### The Problem with Traditional AI Orchestration

Most AI orchestration tools are **blind**. They execute workflows but provide no insight into:
- âŒ **What went wrong** when responses are incorrect
- âŒ **Why costs spiked** on certain requests
- âŒ **Where hallucinations occur** in the pipeline
- âŒ **How to automatically fix** issues when they happen

### The TraceForge Solution

TraceForge is the **first orchestration platform** that combines:

1. **ğŸ” Deep Observability**: Every RAG retrieval, LLM call, and tool execution is automatically traced with OpenTelemetry
2. **ğŸ“Š Quality Evaluation**: Built-in scoring for faithfulness, relevance, policy compliance, and hallucination detection
3. **ğŸ›¡ï¸ Intelligent Remediation**: Automatic fallback strategies when tools fail, policy violations occur, or quality degrades
4. **ğŸ’° Cost Tracking**: Real-time token usage and cost estimation per request
5. **ğŸ—ï¸ Production-Ready**: Designed for enterprise scale with multi-tenant support

---

## âœ¨ Key Features

### ğŸ”„ Intelligent Orchestration
- **Multi-stage pipeline**: RAG â†’ Tools â†’ LLM â†’ Evaluation â†’ Remediation
- **Graceful degradation**: Continue serving requests even when components fail
- **Request tracking**: Full traceability with unique request IDs

### ğŸ“ˆ Built-in Observability
- **OpenTelemetry integration**: Automatic distributed tracing
- **Custom metrics**: Latency, token usage, costs, quality scores
- **Span attributes**: Rich context on every operation
- **Error tracking**: Automatic error capture and attribution

### ğŸ¯ Quality Evaluation
- **Faithfulness scoring**: Detects when responses don't match context (0-1 scale)
- **Relevance assessment**: Ensures retrieved context matches the query (0-1 scale)
- **Policy risk detection**: Identifies potential compliance violations (0-1 scale)
- **Hallucination detection**: Flags suspicious outputs automatically (0-1 scale)
- **Overall quality score**: Weighted composite (threshold: 0.75 for remediation)
- **Automatic evaluation**: Runs on every request with full observability

### ğŸ›¡ï¸ Automatic Remediation
- **CLARIFICATION**: Requests user clarification when quality is low (`overall < 0.75`)
- **Status degradation**: Root span marked as `DEGRADED` when remediation triggers
- **Observable remediation**: Full tracing and metrics for all remediation actions
- **Configurable thresholds**: Quality threshold (0.75) can be adjusted
- **Future strategies**: SAFE_MODE, FALLBACK_TOOL, RETRY_LLM (coming soon)

### ğŸ’° Cost & Performance Tracking
- **Token monitoring**: Track input, output, and total tokens per request (real Gemini API)
- **Cost estimation**: Real-time USD cost calculation based on actual model pricing
- **Performance metrics**: Latency tracking for every stage (RAG, LLM, evaluation, remediation)
- **Multi-tenant support**: Isolate metrics by tenant ID
- **Model fallback tracking**: Automatic fallback to available models with cost transparency

---

## ğŸ—ï¸ Architecture

TraceForge is built as a **monorepo** using pnpm workspaces, enabling code sharing and independent versioning across packages.

```
traceforge/
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ api/              # NestJS REST API server
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ core/             # Orchestration engine
â”‚   â”œâ”€â”€ telemetry/        # OpenTelemetry initialization
â”‚   â”œâ”€â”€ evaluator/        # Quality evaluation logic
â”‚   â”œâ”€â”€ llm/              # LLM provider abstractions
â”‚   â”œâ”€â”€ rag/              # RAG retrieval implementations
â”‚   â”œâ”€â”€ remediation/      # Remediation strategies
â”‚   â”œâ”€â”€ tools/            # Tool execution framework
â”‚   â””â”€â”€ config/           # Shared configuration
```

### Technology Stack

- **Runtime**: Node.js with TypeScript
- **Framework**: NestJS (for API layer)
- **Build Tool**: tsup (for packages)
- **Observability**: OpenTelemetry SDK â†’ OTLP â†’ Datadog Agent (Sidecar) â†’ Datadog Cloud
- **LLM Provider**: Google Gemini (real API integration, gemini-2.5-flash)
- **Vector Database**: Qdrant (keyword search, embeddings coming soon)
- **Package Manager**: pnpm workspaces
- **Deployment**: Google Cloud Run (Serverless)

---

## ğŸš€ Quick Start

### Prerequisites

- Node.js 18+ 
- pnpm 10.26+
- Docker (for Qdrant vector database)
- Datadog account (for observability)

### Installation

```bash
# Clone the repository
git clone https://github.com/rasike-dev/traceforge.git
cd traceforge

# Install dependencies
pnpm install

# Build all packages
pnpm -r build
```

### Environment Setup

1. **Get API Keys**:

   **Google Gemini API Key**:
   - Go to [Google AI Studio](https://makersuite.google.com/app/apikey)
   - Create a new API key
   - Copy the key

   **Datadog API Key**:
   - Go to [Datadog API Keys](https://app.datadoghq.com/organization-settings/api-keys)
   - Create a new API key
   - Copy the key

2. **Create `.env` file** in the project root:

```bash
# Required: Google Gemini API Key
GEMINI_API_KEY=your-gemini-api-key-here

# Required: Datadog API Key (for Datadog Agent)
DD_API_KEY=your-datadog-api-key-here

# Optional: OpenTelemetry Configuration
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
OTEL_METRIC_EXPORT_INTERVAL_MS=5000
NODE_ENV=development
```

2. **Start Qdrant (Vector Database)**:

```bash
# Run Qdrant in Docker
docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant

# Seed Qdrant with demo documents
cd packages/rag
pnpm tsx src/qdrant/seed.ts
```

3. **Start Datadog Agent** (for observability):

```bash
# Start Datadog Agent with OpenTelemetry receiver
docker run -d \
  --name traceforge-datadog-agent \
  -e DD_API_KEY=your_datadog_api_key \
  -e DD_SITE=datadoghq.com \
  -e DD_OTLP_CONFIG_RECEIVER_PROTOCOLS_HTTP_ENDPOINT=0.0.0.0:4318 \
  -p 4318:4318 \
  -p 8126:8126 \
  gcr.io/datadoghq/agent:latest

# Or use docker-compose (if available)
docker-compose -f docker-compose.local.yml up -d
```

**Note**: The Datadog Agent will receive traces and metrics on `localhost:4318` (OTLP endpoint).

### Running the API

```bash
# Set environment variables for OpenTelemetry
export TELEMETRY_PROVIDER=opentelemetry
export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
export OTEL_SERVICE_NAME=traceforge-api
export OTEL_METRIC_EXPORT_INTERVAL_MS=5000

# Start the API server (development mode)
pnpm --filter api run start:dev

# The API will be available at http://localhost:3000
```

**Note**: Make sure Qdrant and Datadog Agent are running before starting the API.

### Production Deployment

**ğŸŒ Live Production Service:**
- **URL:** https://traceforge-api-oymutya24a-uc.a.run.app
- **Platform:** Google Cloud Run (Serverless)
- **Region:** us-central1
- **Status:** âœ… Live and Operational

**Architecture:**
- OpenTelemetry SDK â†’ OTLP (localhost:4318) â†’ Datadog Agent Sidecar â†’ Datadog Cloud
- Real Gemini LLM integration (gemini-2.5-flash)
- Real Qdrant RAG backend
- Full observability (traces + metrics exported every 5 seconds)

**Test Production:**
```bash
# Health check
curl https://traceforge-api-oymutya24a-uc.a.run.app/health

# Send a request
curl -X POST https://traceforge-api-oymutya24a-uc.a.run.app/v1/ask \
  -H "Content-Type: application/json" \
  -d '{"input": "What is observability?", "tenant": "production-test"}'
```

**View Observability in Datadog:**
- **APM Service:** https://app.datadoghq.com/apm/service/traceforge-api
- **Environment:** `production`
- **Filter:** `env:production`

For detailed production information and judge submission details, see [JUDGE_SUBMISSION_INFO.md](./JUDGE_SUBMISSION_INFO.md).

---

## ğŸ“¡ API Reference

### Health Check

```bash
GET /health
```

Returns: `{ "ok": true }`

### Ask Endpoint

```bash
POST /v1/ask
Content-Type: application/json

{
  "input": "What's the weather today?",
  "tenant": "acme-corp"  # optional
}
```

**Query Parameters** (for testing):
- `breakTool=true` - Simulate tool failure
- `badRag=true` - Simulate poor RAG retrieval
- `policyRisk=true` - Simulate policy violation
- `tokenSpike=true` - Simulate high token usage

**Note**: All parameters are optional. The system will use real Gemini API and Qdrant RAG by default.

**Response**:

```json
{
  "requestId": "550e8400-e29b-41d4-a716-446655440000",
  "answer": "The weather today is sunny with a high of 75Â°F...",
  "scores": {
    "faithfulness": 0.88,
    "relevance": 0.9,
    "policy_risk": 0.1,
    "hallucination": 0.12,
    "overall": 0.85
  },
  "meta": {
    "modelName": "gemini-2.5-flash",
    "inputTokens": 200,
    "outputTokens": 120,
    "totalTokens": 320,
    "costUsdEstimate": 0.00032,
    "remediation": null,
    "ragDocs": 3
  }
}
```

**Response with Remediation** (when quality is low):

```json
{
  "requestId": "550e8400-e29b-41d4-a716-446655440001",
  "answer": "I may not have enough reliable information. Could you clarify or provide more details?",
  "scores": {
    "faithfulness": 0.45,
    "relevance": 0.5,
    "policy_risk": 0.1,
    "hallucination": 0.55,
    "overall": 0.65
  },
  "meta": {
    "modelName": "gemini-2.5-flash",
    "inputTokens": 180,
    "outputTokens": 95,
    "totalTokens": 275,
    "costUsdEstimate": 0.00028,
    "remediation": "CLARIFICATION",
    "ragDocs": 0
  }
}
```

---

## ğŸ”¬ Understanding the Orchestration Flow

```
User Request
    â†“
[RAG Retrieval] â”€â”€â”€â†’ Context + Docs Count
    â†“
[Tool Execution] â”€â”€â”€â†’ Tool Results (or failure)
    â†“
[LLM Generation] â”€â”€â”€â†’ Answer + Token Usage + Cost
    â†“
[Evaluation] â”€â”€â”€â†’ Quality Scores (faithfulness, relevance, policy risk, hallucination)
    â†“
[Remediation] â”€â”€â”€â†’ Apply fixes if needed (safe mode, fallback, clarification)
    â†“
Response to User
```

Every stage is:
- âœ… **Traced** with OpenTelemetry spans
- âœ… **Monitored** with custom metrics
- âœ… **Scored** for quality
- âœ… **Remediated** if issues are detected

---

## ğŸ¨ What Makes TraceForge Unique?

### 1. **Observability-First Design**

Unlike LangChain, LlamaIndex, or other orchestration frameworks that treat observability as an afterthought, TraceForge **bakes observability into the core**. Every operation is automatically traced, metered, and logged.

### 2. **Quality Evaluation Built-In**

Traditional tools require you to build custom evaluation logic. TraceForge provides **production-ready evaluation** that runs on every request, scoring:
- Faithfulness (how well the answer matches context)
- Relevance (how well context matches the query)
- Policy compliance (safety and compliance checks)
- Hallucination detection (flagging suspicious outputs)

### 3. **Intelligent Remediation**

When something goes wrong, TraceForge doesn't just failâ€”it **automatically tries to fix it**:
- Tool failures â†’ Fallback to alternative tools
- Policy violations â†’ Safe mode activation
- Low quality â†’ Request clarification from user

### 4. **Multi-Tenant Ready**

Built with enterprise needs in mind, TraceForge supports:
- Tenant isolation
- Per-tenant metrics
- Per-tenant cost tracking
- Per-tenant quality monitoring

### 5. **Cost Transparency**

Every request provides:
- Exact token counts (input/output/total)
- USD cost estimates
- Performance metrics
- Quality scores

This enables **data-driven optimization** and **cost accountability**.

---

## ğŸ› ï¸ Development

### Building Packages

```bash
# Build all packages
pnpm -r build

# Build a specific package
cd packages/core
pnpm build

# Watch mode for development
pnpm dev
```

### Testing the Demo Scenario

Run the included demo script to see good vs. bad paths with remediation:

```bash
# Run demo scenario (good path + bad path with remediation)
./demo-scenario.sh
```

This script demonstrates:
- âœ… **Good path**: High quality response â†’ `status=OK`, no remediation
- âš ï¸ **Bad path**: Low quality response â†’ `status=DEGRADED`, `remediation=CLARIFICATION`

### Running Tests

```bash
# Run API tests
cd apps/api
pnpm test

# Run with coverage
pnpm test:cov

# E2E tests
pnpm test:e2e
```

### Code Quality

```bash
# Lint
pnpm lint

# Format
pnpm format
```

---

## ğŸ“¦ Package Details

### `@traceforge/core`

The orchestration engine that coordinates RAG, tools, LLM, evaluation, and remediation. This is the heart of TraceForge.

**Key exports**:
- `orchestrate()` - Main orchestration function
- Types: `AskRequest`, `AskResponse`, `EvalScores`

### `@traceforge/telemetry`

OpenTelemetry initialization and configuration with vendor-neutral abstraction. Supports both Datadog native tracer and OpenTelemetry SDK.

**Key exports**:
- `initTelemetry()` - Initialize telemetry provider (auto-detects or uses TELEMETRY_PROVIDER)
- Provider abstraction for switching between Datadog and OpenTelemetry

**Features**:
- Loosely coupled architecture (can switch observability vendors)
- OpenTelemetry SDK with OTLP export
- Automatic provider detection (DD_API_KEY â†’ Datadog, else OpenTelemetry)
- Continuous metrics export (configurable interval, default 5 seconds)

### `@traceforge/evaluator`

Deterministic quality evaluation system that scores LLM responses on multiple dimensions.

**Key exports**:
- `basicEvaluate()` - Evaluates responses for faithfulness, relevance, policy risk, hallucination, and overall quality
- Types: `BasicEvaluationInput`, `BasicEvaluationScores`

**Evaluation Dimensions**:
- **Faithfulness**: How well the answer matches retrieved context
- **Relevance**: How well the answer matches the query
- **Policy Risk**: Detection of unsafe or non-compliant content
- **Hallucination**: Likelihood of fabricated information
- **Overall**: Weighted composite score (threshold: 0.75 for remediation)

### `@traceforge/llm`

LLM provider implementations with real Google Gemini integration.

**Key exports**:
- `GeminiProvider` - Real Google Gemini API integration
- Automatic model fallback (gemini-2.5-flash â†’ gemini-2.0-flash â†’ gemini-1.5-pro)
- Token usage tracking and cost calculation

**Supported Models**:
- `gemini-2.5-flash` (default, fast and cost-effective)
- `gemini-2.0-flash` (fallback)
- `gemini-1.5-pro` (fallback for complex tasks)
- `gemini-pro` (legacy fallback)

### `@traceforge/rag`

RAG retrieval implementations with real Qdrant vector database integration.

**Key exports**:
- `QdrantRagProvider` - Real Qdrant integration for document retrieval
- Keyword-based search (Phase 1.2, embeddings coming soon)
- Automatic collection initialization and seeding

**Features**:
- Local Qdrant Docker setup
- 20 pre-seeded demo documents
- Keyword matching with relevance scoring
- Top-K document retrieval

### Other Packages

- **remediation**: Remediation strategy implementations (CLARIFICATION currently implemented)
- **tools**: Tool execution framework (mock implementations, ready for real integrations)

---

## ğŸ¯ Use Cases

TraceForge is perfect for:

- âœ… **Enterprise AI Applications**: Need observability, compliance, and reliability
- âœ… **Production AI Services**: Require quality guarantees and cost tracking
- âœ… **Multi-Tenant SaaS**: Need tenant isolation and per-tenant metrics
- âœ… **High-Stakes Applications**: Where failures are costly and quality matters
- âœ… **Compliance-Critical Systems**: Policy enforcement and auditability required

---

## ğŸ“Š Observability & Monitoring

TraceForge provides comprehensive observability out of the box with pre-configured Datadog dashboards, monitors, and SLOs. The system uses **OpenTelemetry SDK** with **OTLP protocol** for vendor-neutral observability, forwarding to Datadog via a sidecar agent pattern.

### Observability Architecture

**Local Development:**
```
TraceForge API â†’ OTLP (localhost:4318) â†’ Datadog Agent (Docker) â†’ Datadog Cloud
```

**Production (Cloud Run):**
```
TraceForge API â†’ OTLP (localhost:4318) â†’ Datadog Agent (Sidecar) â†’ Datadog Cloud
```

**Key Features:**
- âœ… Vendor-neutral (OpenTelemetry SDK, not locked to Datadog)
- âœ… Continuous metrics export (every 5 seconds)
- âœ… Real-time trace export (on request completion)
- âœ… Automatic provider detection
- âœ… Can switch observability vendors by changing exporter endpoint

### Distributed Tracing

Every request creates a complete trace with spans for:
- `traceforge.request` - Root span for the entire request
- `traceforge.rag` - RAG retrieval stage
- `traceforge.tool` - Tool execution stage
- `traceforge.llm` - LLM generation stage
- `traceforge.evaluation` - Quality evaluation stage
- `traceforge.remediation` - Remediation stage (if triggered)

### Custom Metrics

All metrics are exported to Datadog via OpenTelemetry OTLP protocol. Metrics are continuously pushed every 5 seconds (configurable via `OTEL_METRIC_EXPORT_INTERVAL_MS`):

**Request Metrics**:
- `traceforge.request.count` - Total requests handled
- `traceforge.request.latency_ms` - End-to-end request latency (histogram)
- `traceforge.request.quality_ok` - Requests meeting quality threshold (â‰¥ 0.75)

**RAG Metrics**:
- `traceforge.rag.latency_ms` - RAG retrieval latency (histogram)
- `traceforge.rag.docs.count` - Number of documents retrieved (up/down counter)

**LLM Metrics**:
- `traceforge.llm.latency_ms` - LLM generation latency (histogram)
- `traceforge.llm.tokens.input` - Input tokens consumed (counter)
- `traceforge.llm.tokens.output` - Output tokens generated (counter)
- `traceforge.llm.cost.usd` - Estimated cost in USD (counter)

**Evaluation Metrics**:
- `traceforge.eval.score` - Evaluation scores (up/down counter) with dimension tags: `faithfulness`, `relevance`, `policy_risk`, `hallucination`, `overall`

**Remediation Metrics**:
- `traceforge.remediation.triggered` - Remediation actions triggered (counter) with action tags: `CLARIFICATION`, `SAFE_MODE`, `FALLBACK_TOOL`, `RETRY_LLM`

**Tool Metrics**:
- `traceforge.tool.calls` - Successful tool calls (counter)
- `traceforge.tool.errors` - Tool call errors (counter)
- `traceforge.tool.latency_ms` - Tool execution latency (histogram)

### Service Level Objectives (SLOs)

TraceForge includes comprehensive SLO tracking with 9 pre-configured SLOs in Datadog:

**Performance SLOs**:
- **Request Availability** (7d, 99.5% target): Percentage of requests with status OK or DEGRADED
- **Request Latency P95** (30d, 95% target): Time slices where p95 latency â‰¤ 2000ms
- **Endpoint Latency** (7d, 99.9% target): P95 latency < 1s for `/v1/ask` route

**Quality SLOs**:
- **Response Quality** (30d, 95% target): Percentage of responses with overall quality score â‰¥ 0.75

**Tool Reliability SLOs** (30d, 99% target each):
- Tool Reliability for `weather*` tools
- Tool Reliability for `search*` tools
- Tool Reliability for `payment*` tools
- Tool Reliability for `document_lookup*` tools
- Tool Reliability for `vector_db*` tools

All SLO configurations are available in `datadog/slos.json` for easy import into Datadog.

---

## ğŸ”® Roadmap

### Phase 1.1: Core Observability âœ… COMPLETE
- âœ… Real Gemini LLM provider integration
- âœ… LLM tracing and metrics (latency, tokens, cost)
- âœ… Evaluation system (faithfulness, relevance, policy risk, hallucination)
- âœ… Remediation system (CLARIFICATION action)
- âœ… OpenTelemetry integration with Datadog
- âœ… Multi-tenant support

### Phase 1.2: Real RAG Integration âœ… COMPLETE
- âœ… Qdrant vector database setup
- âœ… Real RAG provider with keyword search
- âœ… RAG tracing and metrics
- âœ… 20 pre-seeded demo documents

### Phase 1.3: Production Monitoring âœ… COMPLETE
- âœ… Datadog monitors and alerts (cost, quality degradation)
- âœ… Comprehensive SLO tracking (9 SLOs: availability, latency, quality, tool reliability)
- âœ… Pre-configured Datadog dashboards (4 dashboards)
- âœ… Quality SLO metric tracking (`traceforge.request.quality_ok`)

### Phase 1.4: Production Deployment âœ… COMPLETE
- âœ… Google Cloud Run deployment with sidecar pattern
- âœ… OpenTelemetry + Datadog Agent sidecar architecture
- âœ… Continuous metrics export (every 5 seconds)
- âœ… Real-time trace export to Datadog
- âœ… Production service live and operational

### Phase 2: Enhanced RAG (Coming Soon)
- ğŸ”„ Embedding-based semantic search
- ğŸ”„ Hybrid search (keyword + semantic)
- ğŸ”„ Reranking for better relevance
- ğŸ”„ Additional vector database support (Pinecone, Weaviate)

### Phase 3: Advanced Features (Future)
- ğŸ“‹ Additional LLM providers (OpenAI, Anthropic)
- ğŸ“‹ Advanced evaluation models (LLM-as-judge, embedding-based)
- ğŸ“‹ Additional remediation strategies (SAFE_MODE, FALLBACK_TOOL, RETRY_LLM)
- ğŸ“‹ Real tool integrations (custom tool execution)
- ğŸ“‹ Webhook support for external remediation
- ğŸ“‹ Dashboard for observability visualization
- ğŸ“‹ Alerting and notification system
- ğŸ“‹ A/B testing framework
- ğŸ“‹ Cost optimization recommendations

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

MIT License

See [LICENSE](./LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- Built with [NestJS](https://nestjs.com/)
- Observability powered by [OpenTelemetry](https://opentelemetry.io/)
- Package management with [pnpm](https://pnpm.io/)

---

### Datadog Configuration

TraceForge includes pre-configured Datadog resources for immediate observability:

**Monitors** (`datadog/monitors.json`, `datadog/monitors1.json`):
- **LLM Cost Alert**: Monitors average cost per request (warning: $0.02, critical: $0.05)
- **AI Quality Degradation**: Alerts when overall quality score < 0.75

**Dashboards** (`datadog/*.json`):
- **LLM System Overview**: Request volume, latency, token usage, cost, tool metrics
- **Cost & Token Economics**: Cost tracking by model, token usage breakdown
- **Reliability & SLOs**: SLO status, error budgets, availability metrics
- **TraceForensics**: Distributed tracing and span analysis

**Import Instructions**:
1. **Monitors**: Import via Datadog API or UI:
   - `datadog/monitors.json` - LLM Cost Per Request alert
   - `datadog/monitors1.json` - AI Quality Degradation alert
2. **SLOs**: Import `datadog/slos.json` via Datadog SLO API (contains all 9 SLOs)
3. **Dashboards**: Import dashboard JSON files via Datadog Dashboard API or UI

**Monitor Details**:
- **LLM Cost Monitor**: Alerts when average cost per request exceeds thresholds (warning: $0.02, critical: $0.05)
- **Quality Degradation Monitor**: Alerts when overall evaluation score falls below 0.75 threshold

## ğŸ“š Additional Resources

For detailed monitoring and observability setup, see:
- **Datadog Configuration**: All monitors, SLOs, and dashboards in `datadog/` directory
- **Datadog Agent Setup**: `docker-compose.local.yml` for local development
- **Production Deployment**: `cloud-run-service.yaml` for Cloud Run with sidecar
- **Judge Submission Info**: `JUDGE_SUBMISSION_INFO.md` for production service details

### Documentation Files

- **[METRICS_AND_SPANS.md](./METRICS_AND_SPANS.md)** - Complete reference for all metrics and spans
- **[RESPONSE_QUALITY_SLO.md](./RESPONSE_QUALITY_SLO.md)** - Response Quality SLO calculation guide
- **[P95_LATENCY_QUERIES.md](./P95_LATENCY_QUERIES.md)** - P95 latency query examples
- **[LLM_TOKEN_USAGE_QUERIES.md](./LLM_TOKEN_USAGE_QUERIES.md)** - LLM token usage calculation
- **[LLM_TOKEN_SPIKE_EVENTS.md](./LLM_TOKEN_SPIKE_EVENTS.md)** - Token spike detection guide
- **[PROMPT_VS_COMPLETION_TOKENS.md](./PROMPT_VS_COMPLETION_TOKENS.md)** - Token breakdown queries
- **[TOOL_NAMING_CONVENTION.md](./TOOL_NAMING_CONVENTION.md)** - Tool naming for wildcard queries
- **[JUDGE_SUBMISSION_INFO.md](./JUDGE_SUBMISSION_INFO.md)** - Production deployment information for judges

## ğŸ“ Support

For questions, issues, or feature requests, please open an issue on GitHub.

---

**Built with â¤ï¸ for reliable, observable, and production-ready AI applications.**

